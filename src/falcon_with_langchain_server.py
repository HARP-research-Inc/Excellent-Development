# # -*- coding: utf-8 -*-
# """FALCON with LangChain

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1X1ulWEQxE7MNV6qxrmEpXtLZ1nlVYUDQ
# """
# """
# Version:                2.0
# Last Edit:              8/2/2023
# Last Author:            Kevin Scott
# Features to add:
# Memeory persistence     8/2/2023 check
# API Functionality       8/2-3/2023
# """

# import os
# from langchain import HuggingFacePipeline
# from transformers import AutoTokenizer, pipeline, LlamaForCausalLM, AutoModelForCausalLM
# import torch
# from langchain import PromptTemplate,  LLMChain
# import requests
# from torch.nn import DataParallel
# import socket


# server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
# server.bind(("localhost", 5000))
# server.listen()
# # model_path = "here_we_go_again/text-generation-webui/models/psmathur_wizardlm_alpaca_dolly_orca_open_llama_13b" #tiiuae/falcon-40b-instruct
# # model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto')
# # tokenizer = AutoTokenizer.from_pretrained(model_path) 
# # # device = torch.device("cuda")
# # # os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:1024" 
# # # DataParallel(model.to(device))

# # pipeline = pipeline(
# #     "text-generation", #task
# #     model=model,
# #     tokenizer=tokenizer,
# #     # device= -1,
# #     torch_dtype=torch.bfloat16,
# #     trust_remote_code=True,
# #     device_map="auto",
# #     max_length=100,
# #     do_sample=True,
# #     # max_split_size_mb=1024,
# #     top_k=10,
# #     num_return_sequences=1,
# #     eos_token_id=tokenizer.eos_token_id,
    
# # )

# # llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})

# # template = """
# # You are an intelligent chatbot. Help the following question with brilliant answers.
# # Question: {question}
# # Answer:"""
# # prompt = PromptTemplate(template=template, input_variables=["question"])

# # llm_chain = LLMChain(prompt=prompt, llm=llm)

# question = "Explain what is Artificial Intelligence as Nursery Rhymes"
# client, address = server.accept()
# print("Connected to {}".format(address))
# #while true ask for user input on promps using llm_chain
# while True:
#     client.send("Hello, Welcome to the chatbot. Pleasure doing business with you".encode())
#     print(client.recv(1024).decode())

import socket

def start_server():
    host = 'localhost' 
    port = 7860 
    server_socket = socket.socket()
    server_socket.bind((host, port))
    server_socket.listen(1)
    conn, address = server_socket.accept() 
    print("Connection from: " + str(address))
    while True:
        data = conn.recv(1024).decode()
        if not data:
            break
        print("from connected user: " + str(data))
        data = input(' -> ')
        conn.send(data.encode())
    conn.close()

if __name__ == '__main__':
    start_server()